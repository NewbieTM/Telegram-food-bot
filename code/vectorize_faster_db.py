import argparse
import os
import shutil
import logging
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

import torch
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_community.vectorstores import Chroma

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)



CHROMA_PATH = "../db/chroma"
DATA_PATH = "../db/data.txt"
BATCH_SIZE = 1000
MAX_WORKERS = 4


def get_embedding_function():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ HuggingFace.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω.
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {device}")

    embeddings = HuggingFaceEmbeddings(
        model_name="DeepPavlov/rubert-base-cased-sentence-transformer",
        model_kwargs={"device": device}
    )
    return embeddings


def load_documents():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞.
    –ö–∞–∂–¥—ã–π –ø—Ä–æ–¥—É–∫—Ç —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç.
    """
    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {DATA_PATH}")

    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞...")
    document_loader = TextLoader(DATA_PATH, encoding="utf-8")
    documents = document_loader.load()



    def preprocess_text(text):
        text = text.lower()
        text = text.strip()
        return text

    split_docs = []
    for doc in documents:
        lines = doc.page_content.split('\n')
        for line in lines:
            line = preprocess_text(line)
            if line:
                split_docs.append(Document(page_content=line))

    logger.info(f"üîç –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(split_docs)}")
    return split_docs


def split_documents(documents: list[Document]):
    """
    –†–∞–∑–¥–µ–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ —á–∞–Ω–∫–∏ –¥–ª—è –ª—É—á—à–µ–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏.
    """
    logger.info("–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,
        chunk_overlap=40,
        length_function=len,
        # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
    )
    split_docs = text_splitter.split_documents(documents)
    logger.info(f"üîó –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è: {len(split_docs)}")
    return split_docs


def clear_database():
    """
    –û—á–∏—â–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö Chroma.
    """
    if os.path.exists(CHROMA_PATH):
        shutil.rmtree(CHROMA_PATH)
        logger.info(f"üóëÔ∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –ø—É—Ç–∏ {CHROMA_PATH} –±—ã–ª–∞ —É–¥–∞–ª–µ–Ω–∞.")
    else:
        logger.info(f"‚ÑπÔ∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –ø—É—Ç–∏ {CHROMA_PATH} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")


def add_to_chroma(chunks: list[Document], embeddings, batch_size=BATCH_SIZE):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö Chroma –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ—ë.
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.
    """
    logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö Chroma...")
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)

    total_chunks = len(chunks)
    logger.info(f"–ù–∞—á–∞–ª–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è {total_chunks} —á–∞–Ω–∫–æ–≤ –≤ Chroma...")

    for i in tqdm(range(0, total_chunks, batch_size), desc="–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤"):
        batch = chunks[i:i + batch_size]
        try:
            db.add_documents(batch)
            logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {i + len(batch)} –∏–∑ {total_chunks} —á–∞–Ω–∫–æ–≤.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —á–∞–Ω–∫–æ–≤ —Å {i} –ø–æ {i + len(batch)}: {e}")

    db.persist()
    logger.info(f"‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ø–æ –ø—É—Ç–∏ {CHROMA_PATH}.")


def process_batches(chunks, embeddings, batch_size=BATCH_SIZE, max_workers=MAX_WORKERS):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Chroma –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ.
    """
    logger.info("–ù–∞—á–∞–ª–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–æ–≤ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ Chroma...")
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)

    total_chunks = len(chunks)
    futures = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        for i in range(0, total_chunks, batch_size):
            batch = chunks[i:i + batch_size]
            futures.append(executor.submit(db.add_documents, batch))

        for future in tqdm(as_completed(futures), total=len(futures), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–æ–≤"):
            try:
                future.result()
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞: {e}")

    db.persist()
    logger.info(f"‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ø–æ –ø—É—Ç–∏ {CHROMA_PATH}.")


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è —É–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ–º —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.
    """
    parser = argparse.ArgumentParser(description="–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.")
    parser.add_argument("--reset", action="store_true", help="–°–±—Ä–æ—Å–∏—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º.")
    parser.add_argument("--batch_size", type=int, default=BATCH_SIZE, help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
    parser.add_argument("--max_workers", type=int, default=MAX_WORKERS,
                        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
    parser.add_argument("--use_parallel", action="store_true", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É.")
    args = parser.parse_args()

    if args.reset:
        logger.info("‚ú® –û—á–∏—â–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
        clear_database()

    documents = load_documents()
    #chunks = split_documents(documents)
    chunks = documents

    embeddings = get_embedding_function()

    if args.use_parallel:
        process_batches(chunks, embeddings, batch_size=args.batch_size, max_workers=args.max_workers)
    else:
        add_to_chroma(chunks, embeddings, batch_size=args.batch_size)

    logger.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞.")


if __name__ == "__main__":
    main()
